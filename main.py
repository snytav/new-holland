# -*- coding: utf-8 -*-
"""Копия блокнота "FT_TS_Ameland.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xmVfMANupOYjc0AtHCSz22kd3-UaK4uT
"""

import math
import pandas as pd
import autograd.numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
sns.set()



f = open("Depth_12.dat","rt")

lines = f.readlines()
cmap = plt.get_cmap('gnuplot')
colors = [cmap(i) for i in np.linspace(0, 1, 56)]
print(lines)

# fig, ax = plt.subplots()
plt.figure()
colors = ['red', 'black', 'blue', 'brown', 'green','yellow','magenta','grey','cyan']
depth_along_years = []
years = []
distances_along_years = []

plt.figure(figsize=(800,600))
number =0
s = ''.join(lines)
years_list = s.split('}}')     # splitting along years
for y in years_list:

    if y == '\n':
        break
    year_and_depths = y.split('{{')
    year = year_and_depths[0].strip()
    years.append(year)
    depths = year_and_depths[1:]

    dists_and_depths = depths[0].split(',')
    print('dists and depths ', dists_and_depths)
    odds  = dists_and_depths[1::2]
    print('odds ',odds)
    evens = dists_and_depths[::2]
    print('evens ',evens)
    odds_cleared = [s.replace('}', '').replace('[', '').replace('{', '').replace('\n', '') for s in odds]
    evens_cleared = [s.replace('}', '').replace('{', '').replace('\n', '') for s in evens]
    odds_numbers = [float(s.strip()) for s in odds_cleared]
    evens_numbers  = [float(s.strip()) for s in evens_cleared]
    depth_along_years.append(odds_numbers)
    print('depth_along_years ',depth_along_years)

    distances_along_years.append(evens_numbers)
    print('distances_along_years ',distances_along_years)
    plt.plot(np.array(evens_numbers), np.array(odds_numbers), label=str(year), color=colors[number % len(colors)])
    plt.title(str(year)+' Nx = '+str(len(evens_numbers)))
    plt.show()

    plt.legend(loc='best')
    number = number + 1
    qq = 0

plt.xlabel('cross-shore distance')
plt.ylabel('depth')
plt.title('Coast profile')
plt.savefig('profiles_along_all_years.png')
def get_distance_cell_and_factor(d0,dist):
    i = 0
    f = 0.0
    #dist = distances_along_years[0]
    if d0 < np.max(dist) and d0 >= np.min(dist):
       for i,d in enumerate(dist):
           #print(i,d,d0)
           if d <= d0 and d0 <= dist[i+1]:
              f = (d0-d)/(dist[i+1]-dist[i])
              break
    return i,f

import autograd.numpy as np
#from autograd import grad, ja
years = [int(y) for y in years]
years = np.array(years)

# find depth by year y and distance m
def depth(y,m,yrs,dis,dep):
  # print(years[:5])
  i = np.where(yrs == y)
  i = i[0][0]
  print(i)
  dis = dis[i]
  dis = np.array(dis)
  print(type(dis),dis)
  j = np.where(dis == m)
  j = j[0][0]
  print(j)
  d = dep[i][j]
  return d

y = 1968
m = 40
d = depth(y,m,years,distances_along_years,depth_along_years)
print(d)

#limit all the variable to 3 to build the minimizer
NL = 30
y3 = years[:NL]
dis3 = []
for d in distances_along_years[:NL]:
    print(d)
    d = d[:NL]
    print(d)
    dis3.append(d)

dep3 = []
for d in depth_along_years[:NL]:
    print(d)
    d = d[:NL]
    print(d)
    dep3.append(d)

print(y3)
print(dis3)
print(dep3)

import autograd.numpy.random as npr


# random numbers list with the structure matching depth
w = []
for y in dep3:
    ws = []
    for m in y:
        t = npr.randn(1)
        #print(t)
        t = t[0]
        ws.append(t)
    w.append(ws)
W = w

W

# subtract two fancy list structures
def minus(W,dep3,tw,td):
    res = []
    loss = 0.0
    for y,ws in zip(dep3,W):
        res1 = []
        for m,w in zip(y,ws):
            res1.append(tw*m-td*w)
        res.append(res1)
    return res

w1 = minus(W,W,1.0,0.0)
w1

import torch
#loss function
def loss_function(W,dep3):
    loss = 0.0
   # dep3 = np.array(dep3)
    for i,dep1d in enumerate(dep3):
        for j,y in enumerate(dep1d):
            xc = float(i)/dep3.shape[0]
            yc = float(j)/dep3.shape[1]
            w = W
            m_poly =( w[0] + w[1]*xc+w[2]*xc**2+w[3]*xc**3+w[4]*xc**4+
                      +w[5] +w[6]*yc+w[7]*yc**2+w[8]*yc**3+w[9]*yc**4
                      )
            #print(i,j,xc,yc,m_poly.item())
            m = dep3[i][j]
            loss += (m-m_poly)**2
            #print(i,j,xc,yc,m_poly.item(),loss.item())
    return loss

dep3 = np.array(dep3)
dep3 = torch.from_numpy(dep3)
W = torch.randn(10)
loss = loss_function(W,dep3)

print('loss ',loss)

def polynom(W,dep3):
    loss = 0.0
    # dep3 = np.array(dep3)
    poly_func = torch.zeros_like(dep3)

    for i,dep1d in enumerate(dep3):
        for j,y in enumerate(dep1d):
            xc = float(i)/dep3.shape[0]
            yc = float(j)/dep3.shape[1]
            w = W
            poly_func[i][j] =( w[0] + w[1]*xc+w[2]*xc**2+w[3]*xc**3+w[4]*xc**4+
                      +w[5] +w[6]*yc+w[7]*yc**2+w[8]*yc**3+w[9]*yc**4
                      )
            #print(i,j,xc,yc,m_poly.item())
            # m = dep3[i][j]
            # loss += (m-m_poly)**2
            #print(i,j,xc,yc,m_poly.item(),loss.item())
    return poly_func

import torch

def f(z,x):
    return x*2+z**3
lmb = 0.1
w = torch.randn(10)
hist = []


w.requires_grad = True
optimizer = torch.optim.Adam([w], lr=0.1, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
#print('w0 ',w)
n=0
lf = 1e6
while lf > 5e2:
    optimizer.zero_grad()
    lf = loss_function(w,dep3)
    lf.backward()
    print(n,lf.item())
    hist.append(lf.item())
   #print('lf1  ',n,lf.item())
    optimizer.step()
    n= n + 1
#gd = grad(loss_function)(w,dep3)
#w = np.array(w)
#gd = np.array(gd)
#print('gd ',gd)
#w = w - lmb*gd
#print(w)
#lf2 = loss_function(w,dep3)
#print('lf2 ',lf2)

hist = np.array(hist)
plt.figure()
plt.plot(hist)
plt.xlabel('number of iteration')
plt.ylabel('Loss function')
plt.plot('loss_vs_epoch.png')



for d in depth_along_years:
    print(len(d),d)

xtic = y3
dis3 = np.array(dis3)
ytic = dis3[1,:]
print('xtic ',xtic)
ytic

X,Y = np.meshgrid(xtic,ytic)

from matplotlib import pyplot as plt
from matplotlib import pyplot, cm
from mpl_toolkits.mplot3d import Axes3D

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
#X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, dep3.numpy(), rstride=1, cstride=1,
cmap=cm.coolwarm,
linewidth=0, antialiased=False)
#ax.set_xlim(0, 1)
#ax.set_ylim(0, 1)
#ax.set_zlim(0, 2)
ax.set_xlabel('years')
ax.set_ylabel('distance,m')
plt.colorbar(surf)
plt.title('Real Depth  Data')
plt.savefig('real_depth.png')

poly_dep3 = polynom(w,dep3)



fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
#X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, poly_dep3.detach().numpy(), rstride=1, cstride=1,
cmap=cm.coolwarm,
linewidth=0, antialiased=False)
#ax.set_xlim(0, 1)
#ax.set_ylim(0, 1)
#ax.set_zlim(0, 2)
ax.set_xlabel('years')
ax.set_ylabel('distance,m')
plt.colorbar(surf)
plt.title('Polynom Approximation of Real Depth  Data')
plt.savefig('polynom_depth.png')

from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error
mae = mean_squared_error(dep3.numpy(),poly_dep3.detach().numpy())

mape = mean_absolute_percentage_error(dep3.numpy(),poly_dep3.detach().numpy())
print(mae,mape)
np.savetxt('coefs.txt',w.detach().numpy())
qq = 0